{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["PyTorch Version:  2.0.1+cu118\n"]}],"source":["import torch\n","import numpy as np\n","import pandas as pd\n","from torch.utils.data import Dataset, DataLoader\n","from multiprocessing import Pool\n","from DTran import DualTran\n","from DTRN2 import DTRN\n","from EEGTran import EEGTran\n","from scipy.signal import butter, filtfilt, iirnotch, lfilter\n","import os\n","from scipy.signal import resample\n","import pywt\n","#from EEGFormer import EEGFormer\n","from pytorch_memlab import profile, set_target_gpu, profile_every\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n","\n","print(\"PyTorch Version: \",torch.__version__)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import torch\n","from torch.utils.data import Dataset\n","# stop using pandas for this it doesn't support multiprocessing\n","\n","class EEGDataset(Dataset):\n","    def __init__(self, dir, chunksize, label_column='label', samples=60000, num_chunks=58):\n","        self.dir = dir\n","        self.chunksize = chunksize\n","        self.label_column = label_column\n","        self.samples = samples\n","        self.n_channels = 128\n","        self.n_timestamps = 500\n","        self.num_chunks = num_chunks\n","        \n","        # Initialize variables for chunk loading\n","        self.current_chunk = []\n","        self.current_labels = []\n","        self.current_row = []\n","        self.chunk_idx = torch.randint(0, 20, (1,)).item()\n","        self.reader = None\n","\n","        # Load the first chunk\n","        self.load_next_chunk()\n","\n","    def load_next_chunk(self):\n","        try:\n","            self.current_chunk = pd.read_parquet(self.dir + f'data_chunk_{self.chunk_idx}.parquet', engine='pyarrow')\n","            self.current_labels = pd.read_parquet(self.dir + f'label_chunk_{self.chunk_idx}.parquet', engine='pyarrow')\n","            print(\"Loaded chunk\", self.chunk_idx)\n","            self.chunk_idx = torch.randint(0, 20, (1,)).item()\n","            \n","        except:\n","            self.chunk_idx = torch.randint(0, 20, (1,)).item()\n","\n","    def __len__(self):\n","        return self.samples\n","\n","    def bandpass_filter(self, data, lowcut, highcut, fs, order=5):\n","        nyq = 0.5 * fs  # Nyquist frequency, which is half of fs\n","        low = lowcut / nyq\n","        high = highcut / nyq\n","        b, a = butter(order, [low, high], btype='band')\n","        y = lfilter(b, a, data)\n","        return y\n","    \n","    def highpass_filter(self, data, cutoff, fs, order=5):\n","        \"\"\"\n","        Apply high-pass filter to data.\n","        \n","        Parameters:\n","        - data: The signal data (numpy array)\n","        - cutoff: Cutoff frequency for high-pass filter\n","        - fs: Sampling frequency of the data\n","        - order: Order of the filter (default is 4)\n","        \n","        Returns:\n","        - Filtered data\n","        \"\"\"\n","        nyq = 0.5 * fs  # Nyquist frequency\n","        normal_cutoff = cutoff / nyq\n","        b, a = butter(order, normal_cutoff, btype='high', analog=False)\n","        y = filtfilt(b, a, data)  # filtfilt is used to apply the filter forwards and backwards to avoid phase shifts\n","        return y\n","    \n","    def __getitem__(self, idx):\n","        if len(self.current_row) == 0:\n","       \n","            self.load_next_chunk()\n","            self.current_row = list(range(len(self.current_chunk)))\n","            np.random.shuffle(self.current_row)\n","            self.curr_row = self.current_row.pop()\n","            \n","\n","        row = self.current_chunk.iloc[self.curr_row].values\n","        label = self.current_labels.iloc[self.curr_row].values\n","        self.curr_row = self.current_row.pop()\n","        \n","        assert row.shape[0] == self.n_channels * self.n_timestamps, \"Unexpected number of columns.\"\n","        row = row.reshape(self.n_channels, self.n_timestamps)\n","\n","        f_sample = 250.0 # Change this to your actual sample frequency\n","        f_notch = 50.0\n","        quality_factor = 30.0  # This defines the width of the notch\n","\n","        # Design the notch filter\n","        b, a = iirnotch(f_notch, quality_factor, f_sample)\n","        filtered_data = np.empty_like(row)\n","        for i in range(row.shape[0]):\n","            filtered_data[i] = filtfilt(b, a, row[i])\n","        \n","        highpass_data = self.highpass_filter(filtered_data, .1, 250.0)\n","        coeffs = pywt.wavedec(highpass_data, 'db4', level=3)\n","        cA3, cD3, cD2, cD1 = coeffs\n","\n","        sigma = np.median(np.abs(cD1)) / 0.6745\n","        n = len(filtered_data)\n","        threshold = sigma * np.sqrt(2 * np.log(n))\n","\n","        #cA3 = pywt.threshold(cA3, threshold*2, mode='soft')\n","        cD3 = pywt.threshold(cD3, threshold*2, mode='soft')\n","        cD2 = pywt.threshold(cD2, threshold*2, mode='soft')\n","        cD1 = pywt.threshold(cD1, threshold*2, mode='soft')\n","\n","        denoised_signal = pywt.waverec([cA3, cD3, cD2, cD1], 'db4')\n","\n","        row = denoised_signal\n","\n","        row = torch.from_numpy(row).float()\n","        label = torch.tensor(label).long();\n","    \n","        return row, label\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded chunk 4\n","Loaded chunk 9\n"]}],"source":["train = 'data\\\\train_data_chunks\\\\'\n","dataset = EEGDataset(train, chunksize=256, label_column='label', samples=120000, num_chunks=59)\n","trainloader = DataLoader(dataset, batch_size=8, shuffle=False, drop_last=True, pin_memory=True)\n","\n","test = 'data\\\\test_data_chunks\\\\'\n","dataset = EEGDataset(test, chunksize=256, label_column='label', samples=20000, num_chunks=10)\n","testloader = DataLoader(dataset, batch_size=8, shuffle=False, drop_last=True, pin_memory=True)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CUDA is available.\n"]}],"source":["if torch.cuda.is_available():\n","    print(\"CUDA is available.\")\n","    device = torch.device(\"cuda:0\")\n","else:\n","    print(\"CUDA is not available.\")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import torch\n","import math\n","import torch.nn as nn\n","\n","class Depthwise1DCNN(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, num_kernels):\n","        super(Depthwise1DCNN, self).__init__()\n","\n","        self.conv1 = nn.Conv1d(in_channels, in_channels * num_kernels, kernel_size, groups=in_channels, padding='valid')\n","        self.layer_norm1 = nn.LayerNorm(491)\n","        self.conv2 = nn.Conv1d(in_channels * num_kernels, in_channels * num_kernels, kernel_size, groups=in_channels, padding='valid')\n","        self.layer_norm2 = nn.LayerNorm(482)\n","        self.conv3 = nn.Conv1d(in_channels * num_kernels, in_channels * num_kernels, kernel_size//2, groups=in_channels, padding='valid')\n","        self.layer_norm3 = nn.LayerNorm(478)\n","        self.relu = nn.ReLU()\n","\n","    @profile_every(1)\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.relu(x)\n","        x = self.layer_norm1(x)\n","        x = self.conv2(x)\n","        x = self.relu(x)\n","        x = self.layer_norm2(x)\n","        x = self.conv3(x)\n","        x = self.relu(x)\n","        x = self.layer_norm3(x)\n","\n","        return x\n","    \n","class RegionalTran(nn.Module):\n","    def __init__(self, d_model, num_heads, num_features, num_blocks):\n","        super(RegionalTran, self).__init__()\n","        self.linear_map = nn.Linear(d_model, d_model)\n","        self.pos_enc = PositionalEncoding(d_model=d_model, max_len=num_features)\n","        self.transformer_block = nn.ModuleList([TransformerBlock(d_model, num_heads) for _ in range(num_blocks)])\n","\n","    @profile_every(1)\n","    def forward(self, x):\n","        print(x.size())\n","        x = self.linear_map(x)\n","        x = self.pos_enc(x)\n","        for transformer in self.transformer_block:\n","            x = transformer(x)\n","        return x\n","    \n","class SynchronousTran(nn.Module):\n","    def __init__(self, d_model, num_heads, num_features, num_blocks):\n","        super(SynchronousTran, self).__init__()\n","        print(d_model)\n","        self.linear_map = nn.Linear(d_model, d_model)\n","        self.pos_enc = PositionalEncoding(d_model=d_model, max_len=num_features)\n","        self.transformer_block = nn.ModuleList([TransformerBlock(d_model, num_heads) for _ in range(num_blocks)])\n","\n","    def forward(self, x):\n","        x = x.permute(0, 2, 1, 3)\n","        x = self.linear_map(x)\n","        x = self.pos_enc(x)\n","        for transformer in self.transformer_block:\n","            x = transformer(x)\n","        return x\n","\n","class TemporalTran(nn.Module):\n","    def __init__(self, d_model, num_heads, num_features, num_blocks, compress=250):\n","        super(TemporalTran, self).__init__()\n","        self.compress = compress\n","        self.linear_map = nn.Linear(d_model, d_model)\n","        self.pos_enc = PositionalEncoding(d_model=d_model, max_len=num_features)\n","        self.transformer_block = nn.ModuleList([TransformerBlock(d_model, num_heads) for _ in range(num_blocks)])\n","    \n","    def compress_temporal(self, x):\n","        B, C, S, D = x.size()\n","\n","        assert D % self.compress == 0\n","\n","        segment_size = D // self.compress\n","\n","        reshaped_x = x.view(B, C, S, D, segment_size)\n","\n","        compressed_x = torch.mean(reshaped_x, dim=-1)\n","\n","        return compressed_x\n","\n","    def forward(self, x):\n","        x = self.compress_temporal(x)\n","        x = x.permute(0, 3, 2, 1)\n","        x = self.linear_map(x)\n","        x = self.pos_enc(x)\n","        for transformer in self.transformer_block:\n","            x = transformer(x)\n","        return x\n","    \n","class Decoder(nn.Module):\n","    def __init__(self, channels, features, temporal, N):\n","        super(Decoder, self).__init__()\n","        self.normConv = nn.Conv2d(features, 1, kernel_size=1)\n","        self.conv2 = nn.Conv2d(in_channels=1, out_channels=N, kernel_size=(5, 5), padding=(2, 2))\n","        self.conv3 = nn.Conv2d(in_channels=N, out_channels=N, kernel_size=(5, 5), padding=(2, 2), stride=(2, 2))\n","        self.linear = nn.Linear(N * (features // 2) * (temporal // 2), 11)\n","        self.relu = nn.ReLU()\n","    \n","    def forward(self, x):\n","        x = x.permute(0, 2, 3, 1)\n","        x = self.normConv(x)\n","        x = self.relu(x)\n","        x = self.conv2(x)\n","        x = self.relu(x)\n","        x = self.conv3(x)\n","        x = self.relu(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.linear(x)\n","        return x\n","\n","class EEGFormer(nn.Module):\n","    def __init__(self, d_model=478, channels=128, features=128, N=128):\n","        super(EEGFormer, self).__init__()\n","        self.channels = channels\n","        self.embedding = Depthwise1DCNN(channels, features, 10, features)\n","        self.regional = RegionalTran(d_model, 8, features, 1)\n","        self.synchronous = SynchronousTran(d_model, 8, features, 1)\n","        self.temporal = TemporalTran(features, 10, features, 1)\n","        self.decoder = Decoder(channels, features, 250, N)\n","        \n","    @profile_every(1)\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x = x.view(x.size(0), self.channels, -1, x.size(-1))\n","        x = self.regional(x)\n","        x = self.synchronous(x)\n","        x = self.temporal(x)\n","        x = self.decoder(x)\n","        return x\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin((position * div_term)*(d_model/max_len))\n","        pe[:, 1::2] = torch.cos((position * div_term)*(d_model/max_len))\n","        pe = pe.unsqueeze(0)\n","\n","        self.register_buffer('pe', pe)\n","\n","    @profile_every(1)\n","    def forward(self, x):\n","        x = x + self.pe[:, :x.size(1), :]\n","        return x\n","    \n","class TransformerBlock(nn.Module):\n","    def __init__(self, d_model, num_heads):\n","        super(TransformerBlock, self).__init__()\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.head_size = d_model // num_heads\n","        self.multi_head_attn = MultiHeadAttention(d_model, num_heads)\n","        self.feed_forward = FeedForward(d_model)\n","        self.layer_norm = nn.LayerNorm(d_model)\n","\n","    @profile_every(1)\n","    def forward(self, x):\n","        x = x + self.multi_head_attn(x)\n","        x = self.layer_norm(x)\n","        x = x + self.feed_forward(x)\n","        x = self.layer_norm(x)\n","        return x\n","    \n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.head_size = d_model // num_heads\n","        self.query = nn.Linear(d_model, d_model)\n","        self.key = nn.Linear(d_model, d_model)\n","        self.value = nn.Linear(d_model, d_model)\n","        self.softmax = nn.Softmax(dim=-1)\n","        self.dropout = nn.Dropout(0.1)\n","        self.projection = nn.Linear(d_model, d_model)\n","\n","    @profile_every(1)\n","    def forward(self, x):\n","\n","        B, S, C, L = x.size()\n","\n","        x = x.view(B*S, C, L)\n","\n","        query = self.query(x)\n","        key = self.key(x)\n","        value = self.value(x)\n","\n","        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_size)\n","        scores = self.softmax(scores)\n","        scores = self.dropout(scores)\n","\n","        context = torch.matmul(scores, value)\n","\n","        context = context.view(B, S, C, L)\n","        \n","        context = self.projection(context)\n","        return context\n","\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, d_model):\n","        super(FeedForward, self).__init__()\n","        self.MLP = nn.Sequential(\n","            nn.Linear(d_model, d_model * 2),\n","            nn.ReLU(),\n","            nn.Linear(d_model * 2, d_model)\n","        )\n","        self.layer_norm = nn.LayerNorm(d_model)\n","\n","    @profile_every(1)\n","    def forward(self, x):\n","        x = x + self.MLP(x)\n","        x = self.layer_norm(x)\n","        return x\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["478\n"]}],"source":["from torch.utils.tensorboard import SummaryWriter\n","import matplotlib.pyplot as plt\n","from focal_loss.focal_loss import FocalLoss\n","import wandb\n","#writer = SummaryWriter()\n","torch.cuda.empty_cache()\n","\n","'''wandb.init(\n","    # set the wandb project where this run will be logged\n","    project=\"my-awesome-project\",\n","    \n","    # track hyperparameters and run metadata\n","    config={\n","    \"learning_rate\": 0.00001,\n","    \"architecture\": \"EEGTran\",\n","    \"dataset\": \"MNSIT-8B\"\n","    }\n",")'''\n","\n","#odel = DualTran().to(device)\n","model = EEGFormer().to(device)\n","#wandb.watch(model, log=\"all\")\n","set_target_gpu(0)\n","\n","#model.load_state_dict(torch.load('model_overfit.pt'))\n","weights = torch.FloatTensor([2, 3.2, 0.7])\n","#criterion = torch.nn.CrossEntropyLoss()\n","criterion = FocalLoss(gamma=0.8)\n","s = torch.nn.Softmax(dim=1)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["import sys\n","def minMaxNorm(tensor):\n","    # Calculate the minimum and maximum values across the seq_len dimension (i.e., dim=-1)\n","    min_vals = torch.min(tensor, dim=2, keepdim=True)[0]\n","    max_vals = torch.max(tensor, dim=2, keepdim=True)[0]\n","\n","    # Perform the min-max normalization\n","    normalized_tensor = (tensor - min_vals) / (max_vals - min_vals + 1e-10)  # Added epsilon to prevent division by zero\n","\n","    return normalized_tensor"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def z_norm(data):\n","    \"\"\"\n","    Apply z-score normalization to EEG data.\n","    \n","    Parameters:\n","    - data: The EEG data (numpy array)\n","    \n","    Returns:\n","    - Normalized data\n","    \"\"\"\n","    mean = torch.mean(data, dim=2, keepdim=True)\n","    std = torch.std(data, dim=2, keepdim=True)\n","    normalized_data = (data - mean) / (std + 1e-10)  # Added epsilon to prevent division by zero\n","    return normalized_data"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded chunk 6\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_memlab\\line_profiler\\line_records.py:184: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  merged[byte_cols] = merged[byte_cols].applymap(readable_size)\n"]},{"name":"stdout","output_type":"stream","text":["## Depthwise1DCNN.forward\n","\n","active_bytes reserved_bytes line code                            \n","         all            all                                      \n","        peak           peak                                      \n","     184.16M        196.00M   17     @profile_every(1)           \n","                              18     def forward(self, x):       \n","     430.16M        442.00M   19         x = self.conv1(x)       \n","     676.16M        688.00M   20         x = self.relu(x)        \n","     677.16M        688.00M   21         x = self.layer_norm1(x) \n","       1.45G          1.46G   22         x = self.conv2(x)       \n","       1.13G          1.46G   23         x = self.relu(x)        \n","       1.13G          1.46G   24         x = self.layer_norm2(x) \n","       1.88G          1.97G   25         x = self.conv3(x)       \n","       1.60G          1.97G   26         x = self.relu(x)        \n","       1.60G          1.97G   27         x = self.layer_norm3(x) \n","                              28                                 \n","       1.60G          1.97G   29         return x                \n","torch.Size([8, 128, 128, 478])\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_memlab\\line_profiler\\line_records.py:184: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  merged[byte_cols] = merged[byte_cols].applymap(readable_size)\n"]},{"name":"stdout","output_type":"stream","text":["## PositionalEncoding.forward\n","\n","active_bytes reserved_bytes line code                                      \n","         all            all                                                \n","        peak           peak                                                \n","       1.84G          1.97G  146     @profile_every(1)                     \n","                             147     def forward(self, x):                 \n","       2.08G          2.21G  148         x = x + self.pe[:, :x.size(1), :] \n","       2.08G          2.21G  149         return x                          \n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_memlab\\line_profiler\\line_records.py:184: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  merged[byte_cols] = merged[byte_cols].applymap(readable_size)\n"]},{"name":"stdout","output_type":"stream","text":["## MultiHeadAttention.forward\n","\n","active_bytes reserved_bytes line code                                                                                    \n","         all            all                                                                                              \n","        peak           peak                                                                                              \n","       1.84G          2.21G  182     @profile_every(1)                                                                   \n","                             183     def forward(self, x):                                                               \n","                             184                                                                                         \n","       1.84G          2.21G  185         B, S, C, L = x.size()                                                           \n","                             186                                                                                         \n","       1.84G          2.21G  187         x = x.view(B*S, C, L)                                                           \n","                             188                                                                                         \n","       2.08G          2.21G  189         query = self.query(x)                                                           \n","       2.31G          2.44G  190         key = self.key(x)                                                               \n","       2.55G          2.68G  191         value = self.value(x)                                                           \n","                             192                                                                                         \n","       2.67G          2.74G  193         scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_size) \n","       2.67G          2.74G  194         scores = self.softmax(scores)                                                   \n","       2.69G          2.74G  195         scores = self.dropout(scores)                                                   \n","                             196                                                                                         \n","       2.92G          2.97G  197         context = torch.matmul(scores, value)                                           \n","                             198                                                                                         \n","       2.92G          2.97G  199         context = context.view(B, S, C, L)                                              \n","                             200                                                                                         \n","       3.16G          3.21G  201         context = self.projection(context)                                              \n","       3.16G          3.21G  202         return context                                                                  \n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_memlab\\line_profiler\\line_records.py:184: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  merged[byte_cols] = merged[byte_cols].applymap(readable_size)\n"]},{"name":"stdout","output_type":"stream","text":["## FeedForward.forward\n","\n","active_bytes reserved_bytes line code                           \n","         all            all                                     \n","        peak           peak                                     \n","       3.39G          3.45G  215     @profile_every(1)          \n","                             216     def forward(self, x):      \n","       4.32G          4.38G  217         x = x + self.MLP(x)    \n","       4.33G          4.38G  218         x = self.layer_norm(x) \n","       4.33G          4.38G  219         return x               \n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_memlab\\line_profiler\\line_records.py:184: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  merged[byte_cols] = merged[byte_cols].applymap(readable_size)\n","c:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_memlab\\line_profiler\\line_records.py:184: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  merged[byte_cols] = merged[byte_cols].applymap(readable_size)\n"]},{"name":"stdout","output_type":"stream","text":["## TransformerBlock.forward\n","\n","active_bytes reserved_bytes line code                                    \n","         all            all                                              \n","        peak           peak                                              \n","       1.84G          2.21G  161     @profile_every(1)                   \n","                             162     def forward(self, x):               \n","       3.39G          3.44G  163         x = x + self.multi_head_attn(x) \n","       3.39G          3.45G  164         x = self.layer_norm(x)          \n","       4.56G          4.61G  165         x = x + self.feed_forward(x)    \n","       4.56G          4.62G  166         x = self.layer_norm(x)          \n","       4.56G          4.62G  167         return x                        \n","## RegionalTran.forward\n","\n","active_bytes reserved_bytes line code                                               \n","         all            all                                                         \n","        peak           peak                                                         \n","       1.60G          1.97G   38     @profile_every(1)                              \n","                              39     def forward(self, x):                          \n","       1.60G          1.97G   40         print(x.size())                            \n","       1.84G          1.97G   41         x = self.linear_map(x)                     \n","       2.08G          2.21G   42         x = self.pos_enc(x)                        \n","       4.56G          4.62G   43         for transformer in self.transformer_block: \n","       4.56G          4.62G   44             x = transformer(x)                     \n","       4.56G          4.62G   45         return x                                   \n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_memlab\\line_profiler\\line_records.py:184: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  merged[byte_cols] = merged[byte_cols].applymap(readable_size)\n"]},{"name":"stdout","output_type":"stream","text":["## PositionalEncoding.forward\n","\n","active_bytes reserved_bytes line code                                      \n","         all            all                                                \n","        peak           peak                                                \n","       5.03G          5.08G  146     @profile_every(1)                     \n","                             147     def forward(self, x):                 \n","       5.26G          5.32G  148         x = x + self.pe[:, :x.size(1), :] \n","       5.26G          5.32G  149         return x                          \n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_memlab\\line_profiler\\line_records.py:184: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  merged[byte_cols] = merged[byte_cols].applymap(readable_size)\n"]},{"name":"stdout","output_type":"stream","text":["## MultiHeadAttention.forward\n","\n","active_bytes reserved_bytes line code                                                                                    \n","         all            all                                                                                              \n","        peak           peak                                                                                              \n","       5.03G          5.32G  182     @profile_every(1)                                                                   \n","                             183     def forward(self, x):                                                               \n","                             184                                                                                         \n","       5.03G          5.32G  185         B, S, C, L = x.size()                                                           \n","                             186                                                                                         \n","       5.03G          5.32G  187         x = x.view(B*S, C, L)                                                           \n","                             188                                                                                         \n","       5.26G          5.32G  189         query = self.query(x)                                                           \n","       5.50G          5.55G  190         key = self.key(x)                                                               \n","       5.73G          5.79G  191         value = self.value(x)                                                           \n","                             192                                                                                         \n","       5.86G          5.91G  193         scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_size) \n","       5.86G          5.91G  194         scores = self.softmax(scores)                                                   \n","       5.87G          5.91G  195         scores = self.dropout(scores)                                                   \n","                             196                                                                                         \n","       6.11G          6.15G  197         context = torch.matmul(scores, value)                                           \n","                             198                                                                                         \n","       6.11G          6.15G  199         context = context.view(B, S, C, L)                                              \n","                             200                                                                                         \n","       6.34G          6.38G  201         context = self.projection(context)                                              \n","       6.34G          6.38G  202         return context                                                                  \n"]},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 478.00 MiB (GPU 0; 8.00 GiB total capacity; 7.04 GiB already allocated; 0 bytes free; 7.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Nathan\\Git\\MNIST_EEG\\train.ipynb Cell 9\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Git/MNIST_EEG/train.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Git/MNIST_EEG/train.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m data \u001b[39m=\u001b[39m z_norm(data)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Git/MNIST_EEG/train.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Git/MNIST_EEG/train.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m label\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(label\u001b[39m.\u001b[39mflatten() \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, torch\u001b[39m.\u001b[39mtensor(\u001b[39m10\u001b[39m), label\u001b[39m.\u001b[39mflatten())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Git/MNIST_EEG/train.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m label \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39mto(device)\n","File \u001b[1;32mc:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_memlab\\line_profiler\\profile.py:100\u001b[0m, in \u001b[0;36mprofile_every.<locals>.inner_decorator.<locals>.run_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_func\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 100\u001b[0m     res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    101\u001b[0m     \u001b[39mif\u001b[39;00m enable:\n\u001b[0;32m    102\u001b[0m         \u001b[39mif\u001b[39;00m func\u001b[39m.\u001b[39mcur_idx \u001b[39m%\u001b[39m output_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n","\u001b[1;32mc:\\Users\\Nathan\\Git\\MNIST_EEG\\train.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Git/MNIST_EEG/train.ipynb#X10sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchannels, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, x\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Git/MNIST_EEG/train.ipynb#X10sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregional(x)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Git/MNIST_EEG/train.ipynb#X10sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msynchronous(x)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Git/MNIST_EEG/train.ipynb#X10sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtemporal(x)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Git/MNIST_EEG/train.ipynb#X10sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x)\n","File \u001b[1;32mc:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","\u001b[1;32mc:\\Users\\Nathan\\Git\\MNIST_EEG\\train.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Git/MNIST_EEG/train.ipynb#X10sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_enc(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Git/MNIST_EEG/train.ipynb#X10sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mfor\u001b[39;00m transformer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_block:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Git/MNIST_EEG/train.ipynb#X10sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     x \u001b[39m=\u001b[39m transformer(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Git/MNIST_EEG/train.ipynb#X10sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n","File \u001b[1;32mc:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_memlab\\line_profiler\\profile.py:100\u001b[0m, in \u001b[0;36mprofile_every.<locals>.inner_decorator.<locals>.run_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_func\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 100\u001b[0m     res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    101\u001b[0m     \u001b[39mif\u001b[39;00m enable:\n\u001b[0;32m    102\u001b[0m         \u001b[39mif\u001b[39;00m func\u001b[39m.\u001b[39mcur_idx \u001b[39m%\u001b[39m output_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n","\u001b[1;32mc:\\Users\\Nathan\\Git\\MNIST_EEG\\train.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Git/MNIST_EEG/train.ipynb#X10sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmulti_head_attn(x)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Git/MNIST_EEG/train.ipynb#X10sZmlsZQ%3D%3D?line=163'>164</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(x)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Git/MNIST_EEG/train.ipynb#X10sZmlsZQ%3D%3D?line=164'>165</a>\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward(x)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Git/MNIST_EEG/train.ipynb#X10sZmlsZQ%3D%3D?line=165'>166</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(x)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Git/MNIST_EEG/train.ipynb#X10sZmlsZQ%3D%3D?line=166'>167</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n","File \u001b[1;32mc:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_memlab\\line_profiler\\profile.py:100\u001b[0m, in \u001b[0;36mprofile_every.<locals>.inner_decorator.<locals>.run_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_func\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 100\u001b[0m     res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    101\u001b[0m     \u001b[39mif\u001b[39;00m enable:\n\u001b[0;32m    102\u001b[0m         \u001b[39mif\u001b[39;00m func\u001b[39m.\u001b[39mcur_idx \u001b[39m%\u001b[39m output_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n","\u001b[1;32mc:\\Users\\Nathan\\Git\\MNIST_EEG\\train.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Git/MNIST_EEG/train.ipynb#X10sZmlsZQ%3D%3D?line=214'>215</a>\u001b[0m \u001b[39m@profile_every\u001b[39m(\u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Git/MNIST_EEG/train.ipynb#X10sZmlsZQ%3D%3D?line=215'>216</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Git/MNIST_EEG/train.ipynb#X10sZmlsZQ%3D%3D?line=216'>217</a>\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mMLP(x)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Git/MNIST_EEG/train.ipynb#X10sZmlsZQ%3D%3D?line=217'>218</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(x)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Git/MNIST_EEG/train.ipynb#X10sZmlsZQ%3D%3D?line=218'>219</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n","File \u001b[1;32mc:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[1;32mc:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\activation.py:103\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n","File \u001b[1;32mc:\\Users\\Nathan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[0;32m   1456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m   1458\u001b[0m \u001b[39mreturn\u001b[39;00m result\n","\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 478.00 MiB (GPU 0; 8.00 GiB total capacity; 7.04 GiB already allocated; 0 bytes free; 7.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["for i in range(10):\n","    total = 0\n","    for j, (data, label) in enumerate(trainloader):\n","        data = data.to(device)\n","        data = z_norm(data)\n","\n","        output = model(data)\n","        label= torch.where(label.flatten() == -1, torch.tensor(10), label.flatten())\n","        \n","        label = label.to(device)\n","        loss = criterion(s(output), label)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        total += loss.item()\n","        '''if np.random.randint(0, 100) == 1:\n","            print(loss.item())\n","            print(one_hot[0])\n","            print(output[0])'''\n","        \n","        #writer.add_scalar('Loss/train', loss.item(), j + i * len(trainloader))\n","        #wandb.log({\"loss/train\": loss.item()})\n","\n","    print(f'epoch{i}', total / len(trainloader))\n","\n","    train_loss = 0\n","    for k, (data, label) in enumerate(testloader):\n","        data = data.to(device)\n","        data = z_norm(data)\n","        output = model(data)\n","    \n","        label = torch.where(label.flatten() == -1, torch.tensor(10), label.flatten())\n","        label = label.to(device)\n","        loss = criterion(s(output), label)\n","\n","        #writer.add_scalar('Loss/test', loss.item(), k + i * len(trainloader))\n","        #wandb.log({\"loss/test\": loss.item()})\n","\n","        train_loss += loss.item()\n","    print(f'test_loss{i}', train_loss / len(testloader))\n","#wandb.finish()\n","    \n","        "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.save(model.state_dict(), 'model_overfit.pt')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
